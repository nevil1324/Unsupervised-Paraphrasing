{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flair"
      ],
      "metadata": {
        "id": "LrQfiq-18QRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210b9b96-4072-4598-e70a-19202b2fa592"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flair in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.10/dist-packages (from flair) (1.34.77)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from flair) (0.3.5)\n",
            "Requirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.5.3)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from flair) (1.2.14)\n",
            "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from flair) (6.2.0)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.7.3)\n",
            "Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from flair) (0.20.3)\n",
            "Requirement already satisfied: janome>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from flair) (0.5.0)\n",
            "Requirement already satisfied: langdetect>=1.0.9 in /usr/local/lib/python3.10/dist-packages (from flair) (1.0.9)\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.9.4)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from flair) (3.7.1)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.10/dist-packages (from flair) (10.1.0)\n",
            "Requirement already satisfied: mpld3>=0.3 in /usr/local/lib/python3.10/dist-packages (from flair) (0.5.10)\n",
            "Requirement already satisfied: pptree>=3.1 in /usr/local/lib/python3.10/dist-packages (from flair) (3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from flair) (2.8.2)\n",
            "Requirement already satisfied: pytorch-revgrad>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from flair) (0.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from flair) (2023.12.25)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair) (1.2.2)\n",
            "Requirement already satisfied: segtok>=1.5.11 in /usr/local/lib/python3.10/dist-packages (from flair) (1.5.11)\n",
            "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from flair) (2.1.0)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from flair) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.66.2)\n",
            "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from flair) (0.3.3)\n",
            "Requirement already satisfied: transformers[sentencepiece]<5.0.0,>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.38.2)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from flair) (1.26.18)\n",
            "Requirement already satisfied: wikipedia-api>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from flair) (0.6.0)\n",
            "Requirement already satisfied: semver<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flair) (3.0.2)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.77 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair) (1.34.77)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (2.31.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (0.1.99)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair) (1.14.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (3.13.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (4.12.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair) (6.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (24.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (3.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mpld3>=0.3->flair) (3.1.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (3.4.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (3.2.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=1.8,>=1.5.0->flair) (12.4.127)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.4.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (3.20.3)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.28.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mpld3>=0.3->flair) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "# Pos tagging of given input sentence using flair pretrained model for ENGLISH\n",
        "def get_Pos_tags_from_sentence(input_sentence):\n",
        "\n",
        "  # load tagger\n",
        "  tagger = SequenceTagger.load(\"flair/pos-english\")\n",
        "\n",
        "  # make example sentence\n",
        "  sentence = Sentence(input_sentence)\n",
        "\n",
        "  # predict POS tags\n",
        "  tagger.predict(sentence)\n",
        "\n",
        "  # print(sentence.tokens)\n",
        "\n",
        "  # create a list to store POS tags\n",
        "  pos_tags = []\n",
        "\n",
        "  # iterate through each token in the sentence and retrieve its POS tag\n",
        "  for token in sentence:\n",
        "      pos_tags.append(token.tag)\n",
        "\n",
        "  # # print the list of POS tags\n",
        "  # print(pos_tags)\n",
        "\n",
        "  return pos_tags"
      ],
      "metadata": {
        "id": "WnP5g74O8B6u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data\n",
        "# Wordnet to get synets of given word\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PXFp0RZdEAn",
        "outputId": "3dde2641-e2b0-49c9-d8b7-91d11bce9c39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.corpus import wordnet\n",
        "from itertools import product\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from heapq import nlargest\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# How much peraphrases to consider at time of generation among all possible combinations\n",
        "percent_random = 0.4\n",
        "# At max generation of peraphrases\n",
        "upper_bound = 4000\n",
        "# How many peraphrases to return\n",
        "top_k_results = 10\n",
        "\n",
        "\n",
        "# Generate percent_random numbers for pick sentences from all possible comibnations.\n",
        "def generate_unique_random_numbers(n):\n",
        "    # Calculate the number of unique numbers to generate (40% of n)\n",
        "    no_of_uniques = math.ceil(percent_random * n)\n",
        "    # print(no_of_uniques)\n",
        "    num_unique_numbers = int(percent_random * n)\n",
        "\n",
        "    no_of_uniques = min(no_of_uniques,upper_bound)\n",
        "\n",
        "    # Generate unique random numbers between 0 and n\n",
        "    unique_random_numbers = random.sample(range(n+1), no_of_uniques)\n",
        "\n",
        "    return unique_random_numbers\n",
        "\n",
        "# Get all the synets for a paticular word using wordnet.\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "          if lemma.name() != word:\n",
        "            synonyms.add(lemma.name())\n",
        "    return list(synonyms)\n",
        "\n",
        "# Make all the possible combinations of sentences from all the synonyms from choosen pos tags\n",
        "def generate_paraphrases(sentence,indices_of_choosen_tags):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    print(tokens)\n",
        "    # synonyms_per_word = [get_synonyms(word) for word in tokens]\n",
        "    synonyms_per_word = [[word] if not get_synonyms(word) or index not in indices_of_choosen_tags else get_synonyms(word) for index,word in enumerate(tokens)]\n",
        "    print(\"Synonyms : \",synonyms_per_word)\n",
        "    paraphrases = []\n",
        "    total_combintions = len(list(product(*synonyms_per_word)))\n",
        "\n",
        "    # # Generate all combinations\n",
        "    # all_combinations = list(product(*synonyms_per_word))\n",
        "\n",
        "    # # Shuffle the list of combinations\n",
        "    # random.shuffle(all_combinations)\n",
        "\n",
        "    print(\"Total Combinations : \",total_combintions)\n",
        "    unique_random_numbers = generate_unique_random_numbers(total_combintions)\n",
        "\n",
        "    print(f\"Considering {len(unique_random_numbers)} peraphrases out of {total_combintions}\")\n",
        "    # Generate possible combination\n",
        "    for index, combination in tqdm(enumerate(product(*synonyms_per_word)), total=len(unique_random_numbers), desc=\"Peraphrase Generation : \"):\n",
        "        # Selecting the sentence if its presnet in random numbers.\n",
        "        if index in unique_random_numbers:\n",
        "          paraphrases.append(' '.join(combination))\n",
        "          # print(paraphrases)\n",
        "    return paraphrases\n",
        "\n",
        "# Calculate similarity of paraphrased sentence with input sentence\n",
        "def compare_to_input(sentence, paraphrases):\n",
        "    reference_sentence = nltk.word_tokenize(sentence)\n",
        "    paraphrase_scores = []\n",
        "\n",
        "    for paraphrase in tqdm(paraphrases,total = len(paraphrases) , desc = \"Comparision\"):\n",
        "        candidate_sentence = nltk.word_tokenize(paraphrase)\n",
        "        similarity_score = semantic_similarity(sentence, paraphrase)  # Pass each sentence individually\n",
        "        paraphrase_scores.append((paraphrase, similarity_score))\n",
        "\n",
        "    # Get the top k paraphrases based on similarity score\n",
        "    top_paraphrases = nlargest(top_k_results, paraphrase_scores, key=lambda x: x[1])\n",
        "\n",
        "    return [paraphrase for paraphrase, score in top_paraphrases]\n",
        "\n",
        "\n",
        "# Load the Universal Sentence Encoder\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "def semantic_similarity(sentence1, sentence2):\n",
        "    # Compute embeddings for the two sentences\n",
        "    embeddings = embed([sentence1, sentence2])\n",
        "\n",
        "    # Calculate cosine similarity between the embeddings\n",
        "    similarity_score = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Choose pos tags whoes synonyms need to find\n",
        "def indices_of_choosen_pos(inp_tag_list):\n",
        "  choosen_tags = [\"NN\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"NNP\",\"NNPS\",\"NNS\",\"UH\",\"JJ\",\"JJR\",\"JJS\",\"VBP\"]\n",
        "\n",
        "  indices_of_tags = []\n",
        "\n",
        "  for index,tag in enumerate(inp_tag_list):\n",
        "    if tag in choosen_tags:\n",
        "      indices_of_tags.append(index)\n",
        "\n",
        "  return indices_of_tags"
      ],
      "metadata": {
        "id": "YujpKERej9MF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = input(\"Enter a sentence: \")\n",
        "pos_tags_of_sentence = get_Pos_tags_from_sentence(input_sentence)\n",
        "print(pos_tags_of_sentence)\n",
        "indices_of_choosen_tags = indices_of_choosen_pos(pos_tags_of_sentence)\n",
        "paraphrases = generate_paraphrases(input_sentence,indices_of_choosen_tags)\n",
        "best_paraphrase = compare_to_input(input_sentence, paraphrases)\n",
        "print()\n",
        "print(\"Top Peraphrases : \")\n",
        "for pera in best_paraphrase:\n",
        "  print(pera)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZzcemN_kG07",
        "outputId": "49d195cb-c673-49ad-b81a-2bb0decc79c6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: Work hard study hard and push your limits\n",
            "2024-04-04 20:27:20,530 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n",
            "['VB', 'RB', 'VB', 'RB', 'CC', 'VB', 'PRP$', 'NNS']\n",
            "['Work', 'hard', 'study', 'hard', 'and', 'push', 'your', 'limits']\n",
            "Synonyms :  [['oeuvre', 'do_work', 'piece_of_work', 'forge', 'make', 'solve', 'form', 'exploit', 'study', 'act', 'ferment', 'exercise', 'play', 'wreak', 'go', 'cultivate', 'function', 'work', 'make_for', 'puzzle_out', 'run', 'workplace', 'body_of_work', 'work_out', 'employment', 'mould', 'turn', 'sour', 'influence', 'act_upon', 'figure_out', 'process', 'work_on', 'crop', 'bring', 'knead', 'mold', 'operate', 'shape', 'put_to_work', 'lick'], ['hard'], ['field', 'report', 'consider', 'analyse', 'subject_field', 'cogitation', 'canvass', 'contemplate', 'canvas', 'subject', 'subject_area', 'meditate', 'examine', 'sketch', 'work', 'learn', 'discipline', 'take', 'analyze', 'hit_the_books', 'bailiwick', 'survey', 'written_report', 'read', 'field_of_study'], ['hard'], ['and'], ['bear_on', 'energy', 'force', 'advertise', 'pushing', 'thrust', 'crusade', 'labour', 'campaign', 'fight', 'button', 'get-up-and-go', 'tug', 'labor', 'crowd', 'drive', 'press', 'promote', 'push_button', 'advertize', 'agitate'], ['your'], ['throttle', 'limit_point', 'define', 'point_of_accumulation', 'specify', 'boundary', 'terminus_ad_quem', 'set', 'trammel', 'restrict', 'limit', 'demarcation', 'restrain', 'circumscribe', 'limitation', 'terminal_point', 'confine', 'fix', 'demarcation_line', 'determine', 'bound']]\n",
            "Total Combinations :  452025\n",
            "Considering 4000 peraphrases out of 452025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Peraphrase Generation : : 452025it [00:21, 21013.97it/s]\n",
            "Comparision: 100%|██████████| 4000/4000 [00:26<00:00, 150.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top Peraphrases : \n",
            "study hard work hard and pushing your limit\n",
            "act hard work hard and pushing your limit\n",
            "work hard field_of_study hard and force your limit\n",
            "study hard work hard and thrust your demarcation_line\n",
            "work hard take hard and drive your specify\n",
            "work hard consider hard and fight your limitation\n",
            "study hard take hard and pushing your restrict\n",
            "work hard work hard and force your point_of_accumulation\n",
            "puzzle_out hard work hard and drive your limit\n",
            "work hard hit_the_books hard and drive your limit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GQCXG3kdvah6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}